{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14501967,"sourceType":"datasetVersion","datasetId":9262352}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brain tumor inference script\n## By: Meet Ghodke","metadata":{}},{"cell_type":"markdown","source":"A webcam video is active for 10 seconds (in this code, this could also be changed as per user requirements). After the conversion of the transfer learning model from tf to tflite, this tflite model is used to capture and classify each frame from the webcam video. After this, the\ninference script is used to allow the inference of model and predict the tumor and confidence in real time.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport time\nimport numpy as np\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T01:17:58.800649Z","iopub.execute_input":"2026-01-15T01:17:58.801839Z","iopub.status.idle":"2026-01-15T01:17:58.806233Z","shell.execute_reply.started":"2026-01-15T01:17:58.801791Z","shell.execute_reply":"2026-01-15T01:17:58.805312Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Interpreter","metadata":{}},{"cell_type":"markdown","source":"This code will use the tensorflow interpreter for the following functionality:\n- Load the Fraunhofer_Image_Classification.tflite model into the code.\n- With an image frame taken from the real time video, build a connection between the frame and the .tflite model for the processing of the image.\n- The .tflite model classifies the model and depicts the confidence rating, use interpreter to output the results.","metadata":{}},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(model_path=\"/kaggle/input/classification-brain-tumor-tflite-model/classification_brain_tumor.tflite\")\ninterpreter.allocate_tensors()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T01:19:24.562024Z","iopub.execute_input":"2026-01-15T01:19:24.562683Z","iopub.status.idle":"2026-01-15T01:19:24.794331Z","shell.execute_reply.started":"2026-01-15T01:19:24.562650Z","shell.execute_reply":"2026-01-15T01:19:24.792991Z"}},"outputs":[{"name":"stderr","text":"INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"- `tf.lite.Interpreter()` is used to load the .tflite model into this program.\n- `.allocate_tensors()` is a method to allocate memory to the input and output of the model via tensors.\n- The input tensor is created to give images captrued via frames from live video as an input to the .tflite model\n- The output tensor is to retrieve the image's confidence score and its classification label.","metadata":{}},{"cell_type":"code","source":"input_index = interpreter.get_input_details()[0]['index']\noutput_index = interpreter.get_output_details()[0]['index']\nprint(\"Input shape desired:\", interpreter.get_input_details()[0]['shape'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T01:20:51.089173Z","iopub.execute_input":"2026-01-15T01:20:51.090343Z","iopub.status.idle":"2026-01-15T01:20:51.096651Z","shell.execute_reply.started":"2026-01-15T01:20:51.090305Z","shell.execute_reply":"2026-01-15T01:20:51.095751Z"}},"outputs":[{"name":"stdout","text":"Input shape desired: [  1 224 224   3]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"- `.get_input_details()` outputs the input tensor dictionary that has details about the image.\n- `.get_output_details()` outputs the predictions in form of a dictionary.\n- MobileNetV2 model has only one input and one output. These methods are used to extract a list of tensor descriptions per input/output.\n- `[0]` picks the first and only description from that dictionary.\n- `['index']` allots a number to the description.","metadata":{}},{"cell_type":"code","source":"labels = [\"Glioma\",\"Meningioma\",\"No tumor\",\"Pituitary\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T01:24:43.579526Z","iopub.execute_input":"2026-01-15T01:24:43.579910Z","iopub.status.idle":"2026-01-15T01:24:43.585687Z","shell.execute_reply.started":"2026-01-15T01:24:43.579882Z","shell.execute_reply":"2026-01-15T01:24:43.584427Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"video_capture = cv2.VideoCapture(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This command opens the webcam from the pc. .VideoCapture(0) method is used to signify the primary (default) camera of the pc.","metadata":{}},{"cell_type":"code","source":"start_seconds = time.time()\ntotal_seconds = 15\nwhile time.time() - start_seconds < total_seconds:\n    ret,frame = video_capture.read()\n    if ret == True:\n        image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        image = cv2.resize(frame,(224,224))\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB) \n        image = image.astype(np.float32) / 255.0\n        image = np.expand_dims(image,axis=0)\n        interpreter.set_tensor(input_index,image)\n        interpreter.invoke()\n        raw_logits = interpreter.get_tensor(output_index)[0]\n        convert_probability = tf.nn.softmax(raw_logits)\n        probability_array = convert_probability.numpy()\n        label = labels[np.argmax(probability_array)]\n        confidence_score = probability_array.max()*100\n        font = cv2.FONT_HERSHEY_PLAIN\n        font_color = (0,0,0)\n        font_size = 2\n        font_thickness = 2\n        cv2.putText(frame, f\"Tumor: {label} , Confidence: {confidence_score:.1f}%\", (30, 30), font, font_size, font_color, font_thickness) \n        cv2.putText(frame, \"By: Meet Ghodke\", (60, 60), font, font_size, font_color, font_thickness)\n        cv2.imshow(\"Inference\", frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    else:\n        print(\"No frame\")\n        break\ncv2.imshow(\"Brain tumor classification\", frame)\ncv2.waitKey(0)\nvideo_capture.release()\ncv2.destroyAllWindows()\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- `time.time()` starts the timer and records the time that video was captured. It startscounting when the webcam starts recording a video.\n- `total_seconds` is the time the user wants to record the video. Here, the user wants torecord this video for 15 seconds.\n- `while time.time() - start_seconds < total_seconds` : Run the loop tocapture frames for analysing until the time the user wants the camera to record.(15seconds).\n- `ret,frame = video_capture.read()` does the following:\n- - `ret` is a variable with boolean data type that returns true if frames from thevideos are available.\n- - `frame` is an image vector in BGR format (default colour of OpenCV).\n- `if ret == True` indicates that the frame was captured.\n- `image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)` converts the RGB format to gray scale (ideal for MRI).\n- `image = cv2.resize(frame,(224,224))` resizes the frame to 224*224 dimensionsthat were similar to the default dimensions of MobileNetV2 model.\n- `image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)` reverses the gray scale image back to RGB format for MobileNetV2 model input.\n- `image = image.astype(np.float32) / 255.0` has two functionalities:\n- - `image.astype(np.float32)`: converts image data type to the data type requiredby tflite for inference (32 bit floating point).\n- - `/255.0`: converts the values from [0,255] to [0,1] for image classification.\n- `image = np.expand_dims(image,axis=0)` adds an extra dimension (number ofbatches). As seen before, the model input desired is [1 , 224 , 224 , 3]\n- `interpreter.set_tensor(input_index,image)` sets the image array to the inputtensor input_index in order to run inference.\n- `interpreter.invoke()` is the method to run the inference.\n- `raw_logits = interpreter.get_tensor(output_index)[0]` generates raw list oflogits from the output tensor.\n- `convert_probability = tf.nn.softmax(raw_logits)` converts the raw scoresobtained from output tensor into probabilities that add up to 1.\n- `probability_array = convert_probability.numpy()` turns the tensorflowoutput tensor array into a numpy array so that its easier to work with.\n- `label = labels[np.argmax(probability_array)]` Returns the highest probability and checks which of the labels provided it corresponds to (0 is Glioma, 1 is Meningioma, 2 is No tumor, 3 is Pituitary)\n- `confidence_score = probability_array.max()*100` plugs the highestprobability out of the probability_array and multiplies by 100 giving the confidence score percentage.\n- `if cv2.waitKey(1) & 0xFF == ord('q'): break` indicates that if key 'q' ispressed before the `total_seconds`, quit the operation.\n- `cv2.waitKey(0)` : When the user presses a key, then close the window.\n- `cap.release() and cv2.destroyAllWindows()` is used while closing the opencv library. It indicates to release webcam resources and close all opencv windows.","metadata":{}},{"cell_type":"code","source":"print(f\"Final Prediction â†’ Tumor: {label}, Confidence: {confidence_score:.1f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}